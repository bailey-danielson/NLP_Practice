{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import text_reader\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from textblob.taggers import NLTKTagger\n",
    "nltk_tagger = NLTKTagger()\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(lines):\n",
    "    \"\"\"Remove numbers and punctuation, and standardize case\n",
    "\n",
    "    Keyword Arguments:\n",
    "    lines: string of text\"\"\"\n",
    "\n",
    "    # import\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # create set of stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    lower_characters = lines.lower()\n",
    "    approved_words = []\n",
    "    white_list = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "\n",
    "    for word in lower_characters.split():\n",
    "        if word not in stop:\n",
    "            clean_word = re.sub(r'[^a-z ]+', '', word)\n",
    "            approved_words.append(clean_word)\n",
    "    return approved_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_remove_stops(lines):\n",
    "    \"\"\"Remove numbers and punctuation, and standardize case\n",
    "\n",
    "    Keyword Arguments:\n",
    "    lines: string of text\"\"\"\n",
    "\n",
    "    # import\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # create set of stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    lower_characters = lines.lower()\n",
    "    approved_words = []\n",
    "    white_list = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "\n",
    "    for word in lower_characters.split():\n",
    "        if word not in stop:\n",
    "            clean_word = re.sub(r'[^a-z ]+', '', word)\n",
    "            approved_words.append(clean_word)\n",
    "    return \" \".join(approved_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words (model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/BaileyDanielson/Documents/Python/NLP_Practice/final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Section</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Index</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Word_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coetzee  In the Heart of the Country.</td>\n",
       "      <td>0</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today my father brought home his new bride.</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>1</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They came clip-clop across  the flats in a dog...</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.225000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Or perhaps they were drawn by two  plumed donk...</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My father wore his black swallowtail  coat and...</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.115079</td>\n",
       "      <td>0.239683</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Section  \\\n",
       "0              Coetzee  In the Heart of the Country.        0   \n",
       "1        Today my father brought home his new bride.        1   \n",
       "2  They came clip-clop across  the flats in a dog...        1   \n",
       "3  Or perhaps they were drawn by two  plumed donk...        1   \n",
       "4  My father wore his black swallowtail  coat and...        1   \n",
       "\n",
       "                    Book_Title        Author  Index  Polarity  Subjectivity  \\\n",
       "0  In the Heart of the Country  J.M. Coetzee      0  0.000000      0.000000   \n",
       "1  In the Heart of the Country  J.M. Coetzee      1  0.136364      0.454545   \n",
       "2  In the Heart of the Country  J.M. Coetzee      2 -0.225000      0.500000   \n",
       "3  In the Heart of the Country  J.M. Coetzee      3  0.000000      1.000000   \n",
       "4  In the Heart of the Country  J.M. Coetzee      4 -0.115079      0.239683   \n",
       "\n",
       "   Word_Count  \n",
       "0           7  \n",
       "1           8  \n",
       "2          25  \n",
       "3          13  \n",
       "4          24  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And then, for a third, there is the new wife, who lies late abed.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for one book only: \n",
    "IHC_sentences = list(df[df[\"Book_Title\"] == \"In the Heart of the Country\"][\"Sentence\"])\n",
    "IHC_sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It wasn’t easy, for to me he looked old, impossibly old, and I could not remember him looking anything other than old – though, in  fact, at that time he could not have been much older than twenty-nine.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL_sentences = list(df[df[\"Book_Title\"] == \"Shadow Lines\"][\"Sentence\"])\n",
    "SL_sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df[\"Sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And then, for a third, there is the new wife, who lies late abed.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'then third new wife lies late abed'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences = [clean_remove_stops(sent) for sent in sentences]\n",
    "clean_sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vector = vectorizer.fit_transform(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9558, 12059)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=2, random_state=42)\n",
    "\n",
    "dtm_nmf = nmf.fit_transform(sent_vector)\n",
    "dtm_nmf = Normalizer(copy=False).fit_transform(dtm_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "would one like could back me see house time us room day it her perhaps old tridib way come go know ila say father away eyes tell him even never little look around much still think though head may man nothing face long must knew grandmother something people last them went then going two mother first told black came years later hands find road left world without hendrik end that whether words every now ever night door behind take hand looked get calcutta life didnt place used woman bed always days seemed moment another make hair often made myself home\n",
      "\n",
      "Topic #1:\n",
      "said ila dont know its me grandmother go may yes right tridib thats head it robi well im you voice now no come going nick that course father there oh little youre hand mrs told time here must then smiling cant remember ill mother turned theres first looking please old price look on wasnt nothing her tell much hes got face calcutta thing think us something didnt mayadebi later him see laughing take malik back story like ive shook everyone laughed eyes saifuddin long looked too place believe asked word heard whether gave really get though rehmanshaheb matter why hair\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf, vectorizer.get_feature_names(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_clean = [clean_remove_stops(sent) for sent in IHC_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_clean = [clean_remove_stops(sent) for sent in SL_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_vector = vectorizer.fit_transform(IHC_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_vector = vectorizer.fit_transform(SL_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf1 = NMF(n_components=5, random_state=42)\n",
    "\n",
    "dtm1_nmf = nmf1.fit_transform(IHC_vector)\n",
    "dtm1_nmf = Normalizer(copy=False).fit_transform(dtm1_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "conakry lutfullah opposite lay jumps screens gazette statesman celebratory hid discovering kindness kissed barking curls label hostel floundering averted famine complaining stern calm edition halfhour officials ducked country alphabet alan\n",
      "\n",
      "Topic #1:\n",
      "inhabit step froze heater rushed bore provoke label edition sipping greying sombre screens been fragrance halfhour eighteenthcentury celebratory kindness inching bachao argument allowed forming indians storm arms pale backboneless betrayed\n",
      "\n",
      "Topic #2:\n",
      "stiff house been recalled scheme barking income church doubtful ducked dust sombre greying hundreds forks linger flown pale stay hotelier pealed soft fraud fuselage fuss schoolwork defeated saying medium pouting\n",
      "\n",
      "Topic #3:\n",
      "leading arms jumps halfhour santoshpur conakry sparkled intimacy jutting conceived inching saris idea step alphabet argument favourite bore say stay pealed leads edition feast majority squirmed saying indeterminate stern retired\n",
      "\n",
      "Topic #4:\n",
      "france allowed ducked storm doubtful bachao squirmed heater headdress curls arms sombre driver celebratory pealed ironic income lay averted palm splintered froze agent freeze failing ramna favourite rushed feel for\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf1, vectorizer.get_feature_names(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf2 = NMF(n_components=3, random_state=42)\n",
    "\n",
    "dtm2_nmf = nmf2.fit_transform(SL_vector)\n",
    "dtm2_nmf = Normalizer(copy=False).fit_transform(dtm2_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "could like back see tridib one us ila house her room time me tell way may around it old little much grandmother told father later go eyes know look went\n",
      "\n",
      "Topic #1:\n",
      "said dont its know go yes grandmother right thats ila im well robi no voice head me may you it now course come going oh that youre there father hand\n",
      "\n",
      "Topic #2:\n",
      "would say go often look grandmother people ask try knew going come think it sometimes him away wonder turn get house always whether every hurry hands find know first happened\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf2, vectorizer.get_feature_names(), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_approved_pos(pos_list, keys_list):\n",
    "    \n",
    "    \"\"\"pos_list = list of (x, y) where x is word and y is part of speech)\n",
    "    keys_list = approved parts of speech\"\"\" \n",
    "    \n",
    "    word_dictionary = {}\n",
    "    for (x , y) in pos_list:\n",
    "        if y in keys_list:\n",
    "            try:\n",
    "                word_dictionary[y].append(x)\n",
    "            except KeyError:\n",
    "                word_dictionary[y] = [x]\n",
    "    return word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_IHC = TextBlob(\" \".join(list(IHC_sentences)), pos_tagger=nltk_tagger)\n",
    "tagged_IHC.pos_tags = tagged_IHC.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"NNP\", \"NN\", \"RB\", \"JJ\", \"NNS\", \"VBN\", \"VBG\", \"NNPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_approved_words = dict_approved_pos(tagged_IHC.pos_tags, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_refined = []\n",
    "\n",
    "for string in IHC_approved_words.values():\n",
    "    IHC_refined.append(string)\n",
    "\n",
    "final_IHC = [' '.join(string) for string in IHC_refined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_SL = TextBlob(\" \".join(list(SL_sentences)), pos_tagger=nltk_tagger)\n",
    "tagged_SL.pos_tags = tagged_SL.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_approved_words = dict_approved_pos(tagged_SL.pos_tags , keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_refined = []\n",
    "\n",
    "for string in SL_approved_words.values():\n",
    "    SL_refined.append(string)\n",
    "\n",
    "final_SL = [' '.join(string) for string in SL_refined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_vector_new = vectorizer.fit_transform(final_IHC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf1 = NMF(n_components=5, random_state=42)\n",
    "\n",
    "dtm1_nmf = nmf1.fit_transform(IHC_vector_new)\n",
    "dtm1_nmf = Normalizer(copy=False).fit_transform(dtm1_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "kembles loaded resented plush kurta khudiram room journalists forehead apprehensively active arc detailed prosperous division affidavits development roar sour annoyed resist cried known gujarati horse singular africa seriousness khwaja politicked personnel deeply afterwards denise incredulously politicos resolved just due ironically drop anti sheath eloquence mossy sashaying afraid misdemeanour aunts brazil\n",
      "\n",
      "Topic #1:\n",
      "door rinse collapse frustration hesitation kerosene judgements idea none attaching crawl soap fingers floodwaters signs solemnly dishevelled halwa battery flowers slogan doctors aquiline easy brixton insulted knuckleduster fragrance politician intelligent bar insomniac administration sides coconut delhi exotic high shopkeeper provocative dhaka postbox raised sold peninsula dates pujas ma francesca drunkard\n",
      "\n",
      "Topic #2:\n",
      "footsteps aisle hame eyes insomniac castellations groin hurt italy analyses clouds again animosity maidservant lanka contempt hanoi justice america indistinctly off hold failcases maps infamous raising desolation drain blow sounds alan enthusiasm shutterless repute lovers sides division aquiline froze heel grandson adding possible demands shoots agent just flasher lots perspectives\n",
      "\n",
      "Topic #3:\n",
      "knowable lanes balances lawyers hazratbal home due escalators horse fashionable factory jove ranjana slapping radiantly married sashayed outsiders coconut ignition pointing judge ironically bijapur deft college cart backwards fasted occasional fixedly labour sons flag sleepy fog headed posters denise aircraft responsible furrows frowned slights buttercup forms mosques button mr mosquito\n",
      "\n",
      "Topic #4:\n",
      "disdainfully soil collected firm restaurant livid cameras dozen and proximity handbags drumming brooding hoarding income shopkeepers paunchy souls early dashboard painstakingly pathological relieved sikh safety help bag hur profession riot phlegm beach response puff indeterminately forthright foundation biddy clock gold jumping coins shrinking battered smiling sobs relatives honey roadside created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf1, vectorizer.get_feature_names(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_vector_new = vectorizer.fit_transform(final_SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf2 = NMF(n_components=5, random_state=42)\n",
    "\n",
    "dtm2_nmf = nmf2.fit_transform(SL_vector_new)\n",
    "dtm2_nmf = Normalizer(copy=False).fit_transform(dtm2_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "not then so back now very there away too even once again just still never only later here as down up really always soon well ever much far often already perhaps quite enough together long suddenly exactly else yet somewhere quickly slowly sometimes almost merely right first anyway usually around\n",
      "\n",
      "Topic #1:\n",
      "grandmother house time room head way father face mother man something hand day nothing voice door car course look kind school place home road family anything moment hair thing morning window bed right garden back part end woman while corner boy evening story wall chair table silence arm world mouth\n",
      "\n",
      "Topic #2:\n",
      "ila tridib may robi calcutta nick mrs price mayadebi dhaka london shaheb road khalil tresawsen don magda india park delhi victoria lane dan lymington england gole saifuddin denise snipe malik west jethamoshai tha pakistan lizzie babu queen january montu maya khulna come street mike god east english kashmir srinagar rehman\n",
      "\n",
      "Topic #3:\n",
      "little old other long few ll much last first own good small ve same re open large next able right black sure many hard thin white whole new big free short indian great high nice real huge quiet such flat only quick young like dark blue deep empty green full\n",
      "\n",
      "Topic #4:\n",
      "been seen gone taken come known called done given made left had heard dressed used grown brought sent written thought decided asked forgotten born expected lived looked surrounded happened changed bought shown puzzled lost spent laughed turned built killed become kept told wanted worried married frightened finished tried shut thrown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf2, vectorizer.get_feature_names(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
