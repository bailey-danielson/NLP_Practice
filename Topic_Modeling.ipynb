{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import text_reader\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from textblob.taggers import NLTKTagger\n",
    "nltk_tagger = NLTKTagger()\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(lines):\n",
    "    \"\"\"Remove numbers and punctuation, and standardize case\n",
    "\n",
    "    Keyword Arguments:\n",
    "    lines: string of text\"\"\"\n",
    "\n",
    "    # import\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # create set of stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    lower_characters = lines.lower()\n",
    "    approved_words = []\n",
    "    white_list = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "\n",
    "    for word in lower_characters.split():\n",
    "        if word not in stop:\n",
    "            clean_word = re.sub(r'[^a-z ]+', '', word)\n",
    "            approved_words.append(clean_word)\n",
    "    return approved_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_remove_stops(lines):\n",
    "    \"\"\"Remove numbers and punctuation, and standardize case\n",
    "\n",
    "    Keyword Arguments:\n",
    "    lines: string of text\"\"\"\n",
    "\n",
    "    # import\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # create set of stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    lower_characters = lines.lower()\n",
    "    approved_words = []\n",
    "    white_list = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "\n",
    "    for word in lower_characters.split():\n",
    "        if word not in stop:\n",
    "            clean_word = re.sub(r'[^a-z ]+', '', word)\n",
    "            approved_words.append(clean_word)\n",
    "    return \" \".join(approved_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words (model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/BaileyDanielson/Documents/Python/NLP_Practice/final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Section</th>\n",
       "      <th>Book_Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Index</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Word_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coetzee  In the Heart of the Country.</td>\n",
       "      <td>0</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today my father brought home his new bride.</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>1</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They came clip-clop across  the flats in a dog...</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.225000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Or perhaps they were drawn by two  plumed donk...</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My father wore his black swallowtail  coat and...</td>\n",
       "      <td>1</td>\n",
       "      <td>In the Heart of the Country</td>\n",
       "      <td>J.M. Coetzee</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.115079</td>\n",
       "      <td>0.239683</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Section  \\\n",
       "0              Coetzee  In the Heart of the Country.        0   \n",
       "1        Today my father brought home his new bride.        1   \n",
       "2  They came clip-clop across  the flats in a dog...        1   \n",
       "3  Or perhaps they were drawn by two  plumed donk...        1   \n",
       "4  My father wore his black swallowtail  coat and...        1   \n",
       "\n",
       "                    Book_Title        Author  Index  Polarity  Subjectivity  \\\n",
       "0  In the Heart of the Country  J.M. Coetzee      0  0.000000      0.000000   \n",
       "1  In the Heart of the Country  J.M. Coetzee      1  0.136364      0.454545   \n",
       "2  In the Heart of the Country  J.M. Coetzee      2 -0.225000      0.500000   \n",
       "3  In the Heart of the Country  J.M. Coetzee      3  0.000000      1.000000   \n",
       "4  In the Heart of the Country  J.M. Coetzee      4 -0.115079      0.239683   \n",
       "\n",
       "   Word_Count  \n",
       "0           7  \n",
       "1           8  \n",
       "2          25  \n",
       "3          13  \n",
       "4          24  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And then, for a third, there is the new wife, who lies late abed.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for one book only: \n",
    "IHC_sentences = list(df[df[\"Book_Title\"] == \"In the Heart of the Country\"][\"Sentence\"])\n",
    "IHC_sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It wasn’t easy, for to me he looked old, impossibly old, and I could not remember him looking anything other than old – though, in  fact, at that time he could not have been much older than twenty-nine.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SL_sentences = list(df[df[\"Book_Title\"] == \"Shadow Lines\"][\"Sentence\"])\n",
    "SL_sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df[\"Sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And then, for a third, there is the new wife, who lies late abed.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'then third new wife lies late abed'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences = [clean_remove_stops(sent) for sent in sentences]\n",
    "clean_sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vector = vectorizer.fit_transform(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9558, 12059)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=2, random_state=42)\n",
    "\n",
    "dtm_nmf = nmf.fit_transform(sent_vector)\n",
    "dtm_nmf = Normalizer(copy=False).fit_transform(dtm_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "would one like could back me see house time us room day it her perhaps old tridib way come go know ila say father away eyes tell him even never little look around much still think though head may man nothing face long must knew grandmother something people last them went then going two mother first told black came years later hands find road left world without hendrik end that whether words every now ever night door behind take hand looked get calcutta life didnt place used woman bed always days seemed moment another make hair often made myself home\n",
      "\n",
      "Topic #1:\n",
      "said ila dont know its me grandmother go may yes right tridib thats head it robi well im you voice now no come going nick that course father there oh little youre hand mrs told time here must then smiling cant remember ill mother turned theres first looking please old price look on wasnt nothing her tell much hes got face calcutta thing think us something didnt mayadebi later him see laughing take malik back story like ive shook everyone laughed eyes saifuddin long looked too place believe asked word heard whether gave really get though rehmanshaheb matter why hair\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf, vectorizer.get_feature_names(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_clean = [clean_remove_stops(sent) for sent in IHC_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_clean = [clean_remove_stops(sent) for sent in SL_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_vector = vectorizer.fit_transform(IHC_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_vector = vectorizer.fit_transform(SL_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf1 = NMF(n_components=5, random_state=42)\n",
    "\n",
    "dtm1_nmf = nmf1.fit_transform(IHC_vector)\n",
    "dtm1_nmf = Normalizer(copy=False).fit_transform(dtm1_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "conakry lutfullah opposite lay jumps screens gazette statesman celebratory hid discovering kindness kissed barking curls label hostel floundering averted famine complaining stern calm edition halfhour officials ducked country alphabet alan\n",
      "\n",
      "Topic #1:\n",
      "inhabit step froze heater rushed bore provoke label edition sipping greying sombre screens been fragrance halfhour eighteenthcentury celebratory kindness inching bachao argument allowed forming indians storm arms pale backboneless betrayed\n",
      "\n",
      "Topic #2:\n",
      "stiff house been recalled scheme barking income church doubtful ducked dust sombre greying hundreds forks linger flown pale stay hotelier pealed soft fraud fuselage fuss schoolwork defeated saying medium pouting\n",
      "\n",
      "Topic #3:\n",
      "leading arms jumps halfhour santoshpur conakry sparkled intimacy jutting conceived inching saris idea step alphabet argument favourite bore say stay pealed leads edition feast majority squirmed saying indeterminate stern retired\n",
      "\n",
      "Topic #4:\n",
      "france allowed ducked storm doubtful bachao squirmed heater headdress curls arms sombre driver celebratory pealed ironic income lay averted palm splintered froze agent freeze failing ramna favourite rushed feel for\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf1, vectorizer.get_feature_names(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf2 = NMF(n_components=3, random_state=42)\n",
    "\n",
    "dtm2_nmf = nmf2.fit_transform(SL_vector)\n",
    "dtm2_nmf = Normalizer(copy=False).fit_transform(dtm2_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "could like back see tridib one us ila house her room time me tell way may around it old little much grandmother told father later go eyes know look went\n",
      "\n",
      "Topic #1:\n",
      "said dont its know go yes grandmother right thats ila im well robi no voice head me may you it now course come going oh that youre there father hand\n",
      "\n",
      "Topic #2:\n",
      "would say go often look grandmother people ask try knew going come think it sometimes him away wonder turn get house always whether every hurry hands find know first happened\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf2, vectorizer.get_feature_names(), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_approved_pos(pos_list, keys_list):\n",
    "    \n",
    "    \"\"\"pos_list = list of (x, y) where x is word and y is part of speech)\n",
    "    keys_list = approved parts of speech\"\"\" \n",
    "    \n",
    "    word_dictionary = {}\n",
    "    for (x , y) in pos_list:\n",
    "        if y in keys_list:\n",
    "            try:\n",
    "                word_dictionary[y].append(x)\n",
    "            except KeyError:\n",
    "                word_dictionary[y] = [x]\n",
    "    return word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_IHC = TextBlob(\" \".join(list(IHC_sentences)), pos_tagger=nltk_tagger)\n",
    "tagged_IHC.pos_tags = tagged_IHC.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"NNP\", \"NN\", \"RB\", \"JJ\", \"NNS\", \"VBN\", \"VBG\", \"NNPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_approved_words = dict_approved_pos(tagged_IHC.pos_tags, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dict_values_list(dictionary):\n",
    "    \n",
    "    \"\"\"Joining values in dictionary that are string values\"\"\"\n",
    "        \n",
    "    dict_refined = []\n",
    "\n",
    "    for string in dictionary.values():\n",
    "        dict_refined.append(string)\n",
    "\n",
    "    final_dict = [' '.join(string) for string in dict_refined]\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_refined = join_dict_values_list(IHC_approved_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_SL = TextBlob(\" \".join(list(SL_sentences)), pos_tagger=nltk_tagger)\n",
    "tagged_SL.pos_tags = tagged_SL.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_approved_words = dict_approved_pos(tagged_SL.pos_tags , keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_refined = join_dict_values_list(SL_approved_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_vector_new = vectorizer.fit_transform(IHC_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf1 = NMF(n_components=5, random_state=42)\n",
    "\n",
    "dtm1_nmf = nmf1.fit_transform(IHC_vector_new)\n",
    "dtm1_nmf = Normalizer(copy=False).fit_transform(dtm1_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "not perhaps then so only now too never here away again back even still far alone ever together yet as there down once just long well also up nowhere sometimes simply else always enough merely somewhere therefore no first much finally asleep very forever really truly already quite before certainly\n",
      "\n",
      "Topic #1:\n",
      "father time day house life nothing night man room bed door woman hand head way world face kitchen body heart wife farm baas floor child moment one hole something money blood miss air water dark end girl light voice stone everything space sun work side earth story place home fire\n",
      "\n",
      "Topic #2:\n",
      "hendrik anna klein god miss come jakob magda must armoede daddy am arthur please ou did kobus noah are mi saturday listen goodnight poor might sunday es femm ca yes annas friday was thank piet water far baas hottentots lee into ah southern enfield vlek amor no has pick sin\n",
      "\n",
      "Topic #3:\n",
      "old other black own last little first full long great good new sure white such possible true second dark many soft next much brown empty dead cold big green same happy open wrong hard whole heavy late spanish enough angry thin human hot wide clean high ready clear red real\n",
      "\n",
      "Topic #4:\n",
      "eyes words days hands things people clothes feet arms stones knees fingers children lips men voices shoulders years flies ears servants shoes teeth ways trees legs birds machines stars times sisters bones thighs stories messages hills hips brothers curtains insects nights daughters walls bodies wings women tears lives toes doors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf1, vectorizer.get_feature_names(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_vector_new = vectorizer.fit_transform(SL_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf2 = NMF(n_components=5, random_state=42)\n",
    "\n",
    "dtm2_nmf = nmf2.fit_transform(SL_vector_new)\n",
    "dtm2_nmf = Normalizer(copy=False).fit_transform(dtm2_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "not then so back now very there away too even once again just still never only later here as down up really always soon well ever much far often already perhaps quite enough together long suddenly exactly else yet somewhere quickly slowly sometimes almost merely right first anyway usually around\n",
      "\n",
      "Topic #1:\n",
      "grandmother house time room head way father face mother man something hand day nothing voice door car course look kind school place home road family anything moment hair thing morning window bed right garden back part end woman while corner boy evening story wall chair table silence arm world mouth\n",
      "\n",
      "Topic #2:\n",
      "ila tridib may robi calcutta nick mrs price mayadebi dhaka london shaheb road khalil tresawsen don magda india park delhi victoria lane dan lymington england gole saifuddin denise snipe malik west jethamoshai tha pakistan lizzie babu queen january montu maya khulna come street mike god east english kashmir srinagar rehman\n",
      "\n",
      "Topic #3:\n",
      "little old other long few ll much last first own good small ve same re open large next able right black sure many hard thin white whole new big free short indian great high nice real huge quiet such flat only quick young like dark blue deep empty green full\n",
      "\n",
      "Topic #4:\n",
      "been seen gone taken come known called done given made left had heard dressed used grown brought sent written thought decided asked forgotten born expected lived looked surrounded happened changed bought shown puzzled lost spent laughed turned built killed become kept told wanted worried married frightened finished tried shut thrown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf2, vectorizer.get_feature_names(), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: using POS dictionary method gives topics of POS values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull all key values from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_IHC = TextBlob(\" \".join(list(IHC_sentences)), pos_tagger=nltk_tagger)\n",
    "tagged_IHC_tags = tagged_IHC.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"NNP\", \"NN\", \"RB\", \"JJ\", \"NNS\", \"VBN\", \"VBG\", \"NNPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_list_approved_pos(tagged_words, keys):\n",
    "\n",
    "    \"\"\"tagged words = list of (x,y) where x is the word and y is the tag,\n",
    "    keys = list of approved parts of speech as strings\"\"\"\n",
    "    new_list = []\n",
    "    \n",
    "    for (x , y) in tagged_words:\n",
    "        if y in keys:\n",
    "            new_list.append(x)\n",
    "    \n",
    "    final_list = [\" \".join(new_list)]\n",
    "    \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_IHC = join_list_approved_pos(tagged_IHC_tags, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_SL = TextBlob(\" \".join(list(SL_sentences)), pos_tagger=nltk_tagger)\n",
    "tagged_SL_tags = tagged_SL.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_SL = join_list_approved_pos(tagged_SL_tags, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHC_vector_final = vectorizer.fit_transform(final_IHC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-88efafc695b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnmf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdtm1_nmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIHC_vector_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdtm1_nmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm1_nmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             shuffle=self.shuffle)\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[0;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         W, H = _initialize_nmf(X, n_components, init=init,\n\u001b[0;32m-> 1011\u001b[0;31m                                random_state=random_state)\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m     l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H = _compute_regularization(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py\u001b[0m in \u001b[0;36m_initialize_nmf\u001b[0;34m(X, n_components, init, eps, random_state)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# extract positive and negative parts of column vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "nmf1 = NMF(n_components=5, random_state=42)\n",
    "\n",
    "dtm1_nmf = nmf1.fit_transform(IHC_vector_final)\n",
    "dtm1_nmf = Normalizer(copy=False).fit_transform(dtm1_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(nmf1, vectorizer.get_feature_names(), 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
